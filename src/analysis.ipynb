{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as pyplot\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from main import run_direct_estimation, run_importance_sampling, run_adaptive_importance_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for estimation methods\n",
    "trial_size_increment = 5\n",
    "max_trial_exp = 2\n",
    "gui = False\n",
    "hill = True\n",
    "policy_file = './best_hill_climbing_policy.pkl'\n",
    "depth = 1000\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run direct estimation\n",
    "# stores n, p_failure, std_error, runtime\n",
    "results_de = {'n':[], 'p_fail':[], 'std_error':[], 'runtime':[]}\n",
    "for i in range(max_trial_exp):\n",
    "    start_time = time.time_ns()\n",
    "    trials = trial_size_increment**(i + 1)\n",
    "    p_failure_de, std_error_de = run_direct_estimation(\n",
    "        trials, \n",
    "        gui,\n",
    "        hill,  # Pass hill climbing flag\n",
    "        policy_file if hill else None  # Pass policy file only if using hill climbing\n",
    "    )\n",
    "    end_time = time.time_ns()\n",
    "    runtime_ns = end_time - start_time\n",
    "    results_de['n'].append(trials)\n",
    "    results_de['p_fail'].append(p_failure_de)\n",
    "    results_de['std_error'].append(std_error_de)\n",
    "    results_de['runtime'].append(runtime_ns)\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Failure Probability: {p_failure_de:.4f} Â± {std_error_de:.4f}\")\n",
    "    print(f\"95% Confidence Interval: [{p_failure_de - 1.96*std_error_de:.4f}, {p_failure_de + 1.96*std_error_de:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running importance sampling with 5 trials and depth 1000...\n",
      "Loaded hill climbing policy from ./best_hill_climbing_policy.pkl\n",
      "Number of samples: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaf25\\anaconda3\\envs\\FPERG\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No failures detected in trajectories.\n",
      "Warning: No failures detected in trajectories.\n",
      "Estimated failure probability: 0.000000\n",
      "\n",
      "Final Results:\n",
      "Estimated Failure Probability: 0.0000\n",
      "95% Confidence Interval: [0.0000, 0.0000]\n",
      "\n",
      "Running importance sampling with 25 trials and depth 1000...\n",
      "Loaded hill climbing policy from ./best_hill_climbing_policy.pkl\n",
      "Number of samples: 25\n",
      "Log weights stats: -4.165656370369106 -0.4376770339997893 3.666620084666647\n",
      "Normalized weights stats: 0.00015818101400366872 0.04 0.39872071586532604\n",
      "Sample of normalized weights: [4.95322004e-03 1.38454789e-02 5.69896992e-03 1.33397859e-02\n",
      " 3.98720716e-01 6.96509298e-03 3.10854675e-03 1.52707327e-02\n",
      " 1.91600877e-03 1.58181014e-04]\n",
      "Pure weighted failure prob: 0.005699\n",
      "Raw failure ratio (unweighted): 0.040000\n",
      "Warning: No failures detected in trajectories.\n",
      "Estimated failure probability: 0.005699\n",
      "\n",
      "Final Results:\n",
      "Estimated Failure Probability: 0.0057\n",
      "95% Confidence Interval: [-0.0031, 0.0145]\n",
      "\n",
      "Running importance sampling with 125 trials and depth 1000...\n",
      "Loaded hill climbing policy from ./best_hill_climbing_policy.pkl\n",
      "Number of samples: 125\n",
      "Log weights stats: -3.9059686531563784 -0.3137184053366218 2.4702269865410926\n",
      "Normalized weights stats: 9.969525918663678e-05 0.007999999999999998 0.058589674955877614\n",
      "Sample of normalized weights: [0.00986554 0.00555641 0.00067238 0.03268047 0.00187583 0.02810078\n",
      " 0.00073739 0.03026439 0.00081379 0.00173658]\n",
      "Pure weighted failure prob: 0.008593\n",
      "Raw failure ratio (unweighted): 0.040000\n",
      "Log weights stats: -4.80673640967143 -0.49779911037631974 3.0001085621461243\n",
      "Normalized weights stats: 3.810310896441068e-05 0.008 0.09363325152710943\n",
      "Sample of normalized weights: [5.57063080e-04 1.18247605e-02 4.26990895e-03 9.01873717e-02\n",
      " 7.73054146e-04 4.46888388e-03 4.61409371e-04 3.81031090e-05\n",
      " 1.00831368e-02 1.29076398e-03]\n",
      "Pure weighted failure prob: 0.037640\n",
      "Raw failure ratio (unweighted): 0.072000\n",
      "Estimated failure probability: 0.023116\n",
      "\n",
      "Final Results:\n",
      "Estimated Failure Probability: 0.0231\n",
      "95% Confidence Interval: [-0.0235, 0.0698]\n"
     ]
    }
   ],
   "source": [
    "# Run importance sampling\n",
    "# stores n, p_failure, std_error, runtime\n",
    "results_ims = {'n':[], 'p_fail':[], 'std_error':[], 'runtime':[]}\n",
    "for i in range(max_trial_exp):\n",
    "    start_time = time.time_ns()\n",
    "    trials = trial_size_increment**(i+1)\n",
    "    failure_prob, std_error = run_importance_sampling(\n",
    "            n_trials=trials,\n",
    "            d=depth,\n",
    "            gui_mode=gui,\n",
    "            use_hill_climbing=hill,\n",
    "            policy_file=policy_file if hill else None\n",
    "        )\n",
    "    end_time = time.time_ns()\n",
    "    runtime_ns = end_time - start_time\n",
    "    results_ims['n'].append(trials)\n",
    "    results_ims['p_fail'].append(failure_prob)\n",
    "    results_ims['std_error'].append(std_error)\n",
    "    results_ims['runtime'].append(runtime_ns)\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Estimated Failure Probability: {failure_prob:.4f}\")\n",
    "    print(f\"95% Confidence Interval: [{failure_prob - 1.96*std_error:.4f}, {failure_prob + 1.96*std_error:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run adaptive importance sampling\n",
    "results_aims = {'n':[], 'p_fail':[], 'std_error':[], 'runtime':[]}\n",
    "for i in range(max_trial_exp):\n",
    "    start_time = time.time_ns()\n",
    "    trials = trial_size_increment**(i+1)\n",
    "    failure_prob, std_error = run_adaptive_importance_sampling(\n",
    "            n_trials=trials,\n",
    "            d=depth,\n",
    "            gui_mode=gui,\n",
    "            use_hill_climbing=hill,\n",
    "            policy_file=policy_file if hill else None\n",
    "        )\n",
    "    end_time = time.time_ns()\n",
    "    runtime_ns = end_time - start_time\n",
    "    results_aims['n'].append(trials)\n",
    "    results_aims['p_fail'].append(failure_prob)\n",
    "    results_aims['std_error'].append(std_error)\n",
    "    results_aims['runtime'].append(runtime_ns)\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Estimated Failure Probability: {failure_prob:.4f}\")\n",
    "    print(f\"95% Confidence Interval: [{failure_prob - 1.96*std_error:.4f}, {failure_prob + 1.96*std_error:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv\n",
    "filename = \"de_results\" + str(time.time_ns()) + \"_depth_\" + str(depth) + \"_max_step_\" + str(trial_size_increment**max_trial_exp)\n",
    "with open(filename, 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(results_de.keys())\n",
    "    writer.writerows(zip(*results_de.values()))\n",
    "\n",
    "filename = \"ims_results_\" + str(time.time_ns()) + \"_depth_\" + str(depth) + \"_max_step_\" + str(trial_size_increment**max_trial_exp)\n",
    "with open(filename, 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(results_ims.keys())\n",
    "    writer.writerows(zip(*results_ims.values()))\n",
    "\n",
    "filename = \"aims_results_\" + str(time.time_ns()) + \"_depth_\" + str(depth) + \"_max_step_\" + str(trial_size_increment**max_trial_exp)\n",
    "with open(filename, 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(results_aims.keys())\n",
    "    writer.writerows(zip(*results_aims.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot estimates with 95% confidence intervals\n",
    "fig, ax = pyplot.subplots(figsize=(12, 6))\n",
    "methods = ['Direct', 'Importance', 'Adaptive']\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "# get values for the largest trial size\n",
    "largest_idx = -1\n",
    "p_vals = [results_de['p_fail'][largest_idx], \n",
    "          results_ims['p_fail'][largest_idx], \n",
    "          results_aims['p_fail'][largest_idx]]\n",
    "errors = [1.96 * results_de['std_error'][largest_idx],\n",
    "          1.96 * results_ims['std_error'][largest_idx],\n",
    "          1.96 * results_aims['std_error'][largest_idx]]\n",
    "\n",
    "bars = ax.bar(x, p_vals, width, yerr=errors, capsize=5, \n",
    "             alpha=0.7, ecolor='black', error_kw={'elinewidth': 2})\n",
    "\n",
    "ax.set_ylabel('Estimated Failure Probability')\n",
    "ax.set_title(f'Comparison of Methods (n={results_de[\"n\"][largest_idx]} trials)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "            f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "pyplot.tight_layout()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Failure Probability', 'Standard Error', 'Runtime (ns)']\n",
    "methods = ['DE', 'IS', 'AIS']\n",
    "trial_sizes = results_de['n']\n",
    "\n",
    "for i, n in enumerate(trial_sizes):\n",
    "    # get metric values for each method at this trial size\n",
    "    p_vals = [results_de['p_fail'][i], results_ims['p_fail'][i], results_aims['p_fail'][i]]\n",
    "    se_vals = [results_de['std_error'][i], results_ims['std_error'][i], results_aims['std_error'][i]]\n",
    "    rt_vals = [results_de['runtime'][i], results_ims['runtime'][i], results_aims['runtime'][i]]\n",
    "    \n",
    "    # normalize the values for better visualization\n",
    "    p_norm = [v / max(p_vals) if max(p_vals) > 0 else 0 for v in p_vals]\n",
    "    se_norm = [min(se_vals) / v if v > 0 else 1 for v in se_vals]\n",
    "    rt_norm = [min(rt_vals) / v if v > 0 else 1 for v in rt_vals]\n",
    "    \n",
    "    # creat data for this trial size\n",
    "    data = []\n",
    "    for j, method in enumerate(methods):\n",
    "        for metric, value in zip(metrics, [p_norm[j], se_norm[j], rt_norm[j]]):\n",
    "            data.append({\n",
    "                'Method': method,\n",
    "                'Metric': metric,\n",
    "                'Value': value\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df_pivot = df.pivot(index='Method', columns='Metric', values='Value')\n",
    "    pyplot.figure(figsize=(10, 5))\n",
    "    ax = sns.heatmap(df_pivot, annot=True, cmap='viridis', fmt='.2f', linewidths=.5)\n",
    "    pyplot.title(f'Normalized Performance Metrics for n={n} trials', fontsize=14)\n",
    "    pyplot.xticks(rotation=30, ha='right')\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig(f'performance_metrics_n{n}.png', dpi=300, bbox_inches='tight')\n",
    "    pyplot.close()  # Close this figure before creating the next one\n",
    "    \n",
    "    print(f\"Created and saved plot for n={n}\")\n",
    "\n",
    "print(\"All plots have been created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure probability graph\n",
    "pyplot.plot(results_de['n'], results_de['p_fail'])\n",
    "pyplot.plot(results_ims['n'], results_ims['p_fail'])\n",
    "pyplot.plot(results_aims['n'], results_aims['p_fail'])\n",
    "pyplot.legend([\"Direct Estimation\", \"Importance Sampling\", \"Adaptive Importance Sampling\"])\n",
    "pyplot.title(\"Failure Probability vs Number of Trials\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard error graph\n",
    "pyplot.plot(results_de['n'], results_de['std_error'])\n",
    "pyplot.plot(results_ims['n'], results_ims['std_error'])\n",
    "pyplot.plot(results_aims['n'], results_aims['std_error'])\n",
    "pyplot.legend([\"Direct Estimation\", \"Importance Sampling\", \"Adaptive Importance Sampling\"])\n",
    "pyplot.title(\"Standard Error vs Number of Trials\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime graph\n",
    "pyplot.plot(results_de['n'], results_de['runtime'])\n",
    "pyplot.plot(results_ims['n'], results_ims['runtime'])\n",
    "pyplot.plot(results_aims['n'], results_aims['runtime'])\n",
    "pyplot.legend([\"Direct Estimation\", \"Importance Sampling\", \"Adaptive Importance Sampling\"])\n",
    "pyplot.title(\"Runtime vs Number of Trials\")\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FPERG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
